optuna:
  n_trials: 20
  cv: 3
  scoring: f1_weighted
  direction: maximize

model_selection:
  module_0:
    class: RandomForestClassifier
    module: sklearn.ensemble
    params:
      random_state: 42
    search_param_grid:
      n_estimators: [150, 200, 300]
      max_depth: [5, 7, 9]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]

  module_1:
    class: GradientBoostingClassifier
    module: sklearn.ensemble
    params:
      random_state: 42
    search_param_grid:
      learning_rate: [0.01, 0.05, 0.1]
      n_estimators: [100, 200, 300]
      max_depth: [3, 5]
      subsample: [0.7, 0.8, 1.0]

  module_2:
    class: XGBClassifier
    module: xgboost
    params:
      objective: binary:logistic
      eval_metric: logloss
      random_state: 42
      use_label_encoder: False
    search_param_grid:
      n_estimators: [150, 200, 250, 300]
      max_depth: [3, 5, 7]
      learning_rate: [0.01, 0.05, 0.1]
      subsample: [0.7, 0.8, 1.0]
      colsample_bytree: [0.6, 0.8, 1.0]

  # Optional: Add more models here
  # module_3:
  #   class: LogisticRegression
  #   module: sklearn.linear_model
  #   params:
  #     random_state: 42
  #   search_param_grid:
  #     C: [0.1, 1.0, 10.0]
  #     penalty: ["l2"]
  #     solver: ["lbfgs", "liblinear"]
