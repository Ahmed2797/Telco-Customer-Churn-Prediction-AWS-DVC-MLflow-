optuna:
  n_trials: 2
  cv: 2
  scoring: f1_weighted
  direction: maximize

model_selection:

  module_0:
    class: RandomForestClassifier
    module: sklearn.ensemble
    params:
      random_state: 42
    search_param_grid:
      n_estimators:
        low: 300
        high: 400
        step: 50
      max_depth:
        low: 5
        high: 10
      min_samples_split:
        low: 2
        high: 10
      min_samples_leaf:
        low: 2
        high: 7

  module_1:
    class: GradientBoostingClassifier
    module: sklearn.ensemble
    params:
      random_state: 42
    search_param_grid:
      learning_rate:
        low: 0.01
        high: 0.1
      n_estimators:
        low: 300
        high: 400
        step: 50
      max_depth:
        low: 3
        high: 10
      subsample:
        low: 0.6
        high: 1.0

  module_2:
    class: XGBClassifier
    module: xgboost
    params:
      objective: binary:logistic
      eval_metric: logloss
      random_state: 42
      use_label_encoder: False
    search_param_grid:
      n_estimators:
        low: 300
        high: 400
        step: 50
      max_depth:
        low: 3
        high: 10
      learning_rate:
        low: 0.01
        high: 0.2
      subsample:
        low: 0.5
        high: 1.0
      colsample_bytree:
        low: 0.6
        high: 1.0







# optuna:
#   n_trials: 20
#   cv: 3
#   scoring: f1_weighted
#   direction: maximize

# model_selection:
#   module_0:
#     class: RandomForestClassifier
#     module: sklearn.ensemble
#     params:
#       random_state: 42
#     search_param_grid:
#       n_estimators: [300,800]
#       max_depth: [5, 10]
#       min_samples_split: [2, 10]
#       min_samples_leaf: [2,7]

#   module_1:
#     class: GradientBoostingClassifier
#     module: sklearn.ensemble
#     params:
#       random_state: 42
#     search_param_grid:
#       learning_rate: [0.01,0.1]
#       n_estimators: [300,800]
#       max_depth: [3,10]
#       subsample: [0.6,1.0]

#   module_2:
#     class: XGBClassifier
#     module: xgboost
#     params:
#       objective: binary:logistic
#       eval_metric: logloss
#       random_state: 42
#       use_label_encoder: False
#     search_param_grid:
#       n_estimators: [300,800]
#       max_depth: [3,10]
#       learning_rate: [0.01,0.2]
#       subsample: [0.5, 1.0]
#       colsample_bytree: [0.6, 1.0]

# #   # Optional: Add more models here
# #   # module_3:
# #   #   class: LogisticRegression
# #   #   module: sklearn.linear_model
# #   #   params:
# #   #     random_state: 42
# #   #   search_param_grid:
# #   #     C: [0.1, 1.0, 10.0]
# #   #     penalty: ["l2"]
# #   #     solver: ["lbfgs", "liblinear"]
